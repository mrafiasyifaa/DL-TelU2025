{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Models & Training with TensorFlow**"
      ],
      "metadata": {
        "id": "MBCsWJtKz2wG"
      },
      "id": "MBCsWJtKz2wG"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ============================================================================\n",
        "# 1. CUSTOM LOSS FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def huber_fn(y_true, y_pred):\n",
        "    \"\"\"Simple Huber loss function\"\"\"\n",
        "    error = y_true - y_pred\n",
        "    is_small_error = tf.abs(error) < 1\n",
        "    squared_loss = tf.square(error) / 2\n",
        "    linear_loss = tf.abs(error) - 0.5\n",
        "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "def create_huber(threshold=1.0):\n",
        "    \"\"\"Factory function to create Huber loss with custom threshold\"\"\"\n",
        "    def huber_fn(y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) < threshold\n",
        "        squared_loss = tf.square(error) / 2\n",
        "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "    return huber_fn\n",
        "\n",
        "class HuberLoss(keras.losses.Loss):\n",
        "    \"\"\"Custom Huber Loss class\"\"\"\n",
        "    def __init__(self, threshold=1.0, **kwargs):\n",
        "        self.threshold = threshold\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) < self.threshold\n",
        "        squared_loss = tf.square(error) / 2\n",
        "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"threshold\": self.threshold}\n",
        "\n",
        "# ============================================================================\n",
        "# 2. CUSTOM FUNCTIONS (ACTIVATION, INITIALIZER, REGULARIZER, CONSTRAINT)\n",
        "# ============================================================================\n",
        "\n",
        "def my_softplus(z):\n",
        "    \"\"\"Custom softplus activation\"\"\"\n",
        "    return tf.math.log(tf.exp(z) + 1.0)\n",
        "\n",
        "def my_glorot_initializer(shape, dtype=tf.float32):\n",
        "    \"\"\"Custom Glorot initializer\"\"\"\n",
        "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
        "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
        "\n",
        "def my_l1_regularizer(weights):\n",
        "    \"\"\"Custom L1 regularizer\"\"\"\n",
        "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
        "\n",
        "def my_positive_weights(weights):\n",
        "    \"\"\"Custom constraint to keep weights positive\"\"\"\n",
        "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
        "\n",
        "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
        "    \"\"\"Custom L1 Regularizer class\"\"\"\n",
        "    def __init__(self, factor):\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, weights):\n",
        "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"factor\": self.factor}\n",
        "\n",
        "# ============================================================================\n",
        "# 3. CUSTOM METRICS\n",
        "# ============================================================================\n",
        "\n",
        "class HuberMetric(keras.metrics.Metric):\n",
        "    \"\"\"Custom Huber metric\"\"\"\n",
        "    def __init__(self, threshold=1.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.threshold = threshold\n",
        "        self.huber_fn = create_huber(threshold)\n",
        "        # Fixed: Pass shape as tuple, not string\n",
        "        self.total = self.add_weight(name=\"total\", shape=(), initializer=\"zeros\")\n",
        "        self.count = self.add_weight(name=\"count\", shape=(), initializer=\"zeros\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        metric = self.huber_fn(y_true, y_pred)\n",
        "        self.total.assign_add(tf.reduce_sum(metric))\n",
        "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"threshold\": self.threshold}\n",
        "\n",
        "# ============================================================================\n",
        "# 4. CUSTOM LAYERS\n",
        "# ============================================================================\n",
        "\n",
        "class MyDense(keras.layers.Layer):\n",
        "    \"\"\"Custom Dense layer\"\"\"\n",
        "    def __init__(self, units, activation=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = keras.activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(\n",
        "            name='kernel',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        self.bias = self.add_weight(\n",
        "            name='bias',\n",
        "            shape=(self.units,),\n",
        "            initializer='zeros',\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, X):\n",
        "        return self.activation(X @ self.kernel + self.bias)\n",
        "\n",
        "    def compute_output_shape(self, batch_input_shape):\n",
        "        # Handle both tuple and TensorShape inputs\n",
        "        if hasattr(batch_input_shape, 'as_list'):\n",
        "            shape_list = batch_input_shape.as_list()\n",
        "        else:\n",
        "            shape_list = list(batch_input_shape)\n",
        "        return tf.TensorShape(shape_list[:-1] + [self.units])\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"units\": self.units,\n",
        "                \"activation\": keras.activations.serialize(self.activation)}\n",
        "\n",
        "class MyGaussianNoise(keras.layers.Layer):\n",
        "    \"\"\"Custom Gaussian Noise layer\"\"\"\n",
        "    def __init__(self, stddev, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.stddev = stddev\n",
        "\n",
        "    def call(self, X, training=None):\n",
        "        if training:\n",
        "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
        "            return X + noise\n",
        "        else:\n",
        "            return X\n",
        "\n",
        "    def compute_output_shape(self, batch_input_shape):\n",
        "        return batch_input_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"stddev\": self.stddev}\n",
        "\n",
        "class ResidualBlock(keras.layers.Layer):\n",
        "    \"\"\"Custom Residual Block\"\"\"\n",
        "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.n_layers = n_layers\n",
        "        self.n_neurons = n_neurons\n",
        "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n",
        "                                        kernel_initializer=\"he_normal\")\n",
        "                      for _ in range(n_layers)]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.hidden:\n",
        "            Z = layer(Z)\n",
        "        return inputs + Z\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"n_layers\": self.n_layers, \"n_neurons\": self.n_neurons}\n",
        "\n",
        "# ============================================================================\n",
        "# 5. CUSTOM MODELS\n",
        "# ============================================================================\n",
        "\n",
        "class ResidualRegressor(keras.Model):\n",
        "    \"\"\"Custom model with residual blocks\"\"\"\n",
        "    def __init__(self, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n",
        "                                        kernel_initializer=\"he_normal\")\n",
        "        self.block1 = ResidualBlock(2, 30)\n",
        "        self.block2 = ResidualBlock(2, 30)\n",
        "        self.out = keras.layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = self.hidden1(inputs)\n",
        "        for _ in range(1 + 3):\n",
        "            Z = self.block1(Z)\n",
        "        Z = self.block2(Z)\n",
        "        return self.out(Z)\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"output_dim\": self.output_dim}\n",
        "\n",
        "class ReconstructingRegressor(keras.Model):\n",
        "    \"\"\"Custom model with reconstruction loss\"\"\"\n",
        "    def __init__(self, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
        "                                        kernel_initializer=\"lecun_normal\")\n",
        "                      for _ in range(5)]\n",
        "        self.out = keras.layers.Dense(output_dim)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        n_inputs = input_shape[-1]\n",
        "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.hidden:\n",
        "            Z = layer(Z)\n",
        "        reconstruction = self.reconstruct(Z)\n",
        "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
        "        self.add_loss(0.05 * recon_loss)\n",
        "        return self.out(Z)\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"output_dim\": self.output_dim}\n",
        "\n",
        "# ============================================================================\n",
        "# 6. CUSTOM GRADIENT FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "@tf.function\n",
        "def f(w1, w2):\n",
        "    \"\"\"Example function for gradient computation\"\"\"\n",
        "    return 3 * w1**2 + 2 * w1 * w2\n",
        "\n",
        "@tf.custom_gradient\n",
        "def my_better_softplus(z):\n",
        "    \"\"\"Custom softplus with custom gradient\"\"\"\n",
        "    exp = tf.exp(z)\n",
        "    def my_softplus_gradients(grad):\n",
        "        return grad / (1 + 1 / exp)\n",
        "    return tf.math.log(exp + 1), my_softplus_gradients\n",
        "\n",
        "# ============================================================================\n",
        "# 7. TRAINING UTILITIES\n",
        "# ============================================================================\n",
        "\n",
        "def random_batch(X, y, batch_size=32):\n",
        "    \"\"\"Generate random batch\"\"\"\n",
        "    idx = np.random.randint(len(X), size=batch_size)\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "def print_status_bar(iteration, total, loss, metrics=None):\n",
        "    \"\"\"Print training progress\"\"\"\n",
        "    metrics_str = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
        "                             for m in [loss] + (metrics or [])])\n",
        "    end = \"\" if iteration < total else \"\\n\"\n",
        "    print(f\"\\r{iteration}/{total} - {metrics_str}\", end=end)\n",
        "\n",
        "# ============================================================================\n",
        "# 8. MAIN TRAINING PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"Creating synthetic dataset...\")\n",
        "    # Create sample data\n",
        "    X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Scale the data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    print(f\"Training data shape: {X_train_scaled.shape}\")\n",
        "    print(f\"Training labels shape: {y_train.shape}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # Example 1: Using functional custom loss\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EXAMPLE 1: Model with custom Huber loss function\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    model1 = keras.Sequential([\n",
        "        keras.layers.Dense(64, activation=\"relu\", input_shape=(X_train_scaled.shape[1],)),\n",
        "        keras.layers.Dense(32, activation=\"relu\"),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    model1.compile(loss=huber_fn, optimizer=\"adam\", metrics=[\"mae\"])\n",
        "\n",
        "    history1 = model1.fit(X_train_scaled, y_train,\n",
        "                         validation_data=(X_test_scaled, y_test),\n",
        "                         epochs=5, batch_size=32, verbose=1)\n",
        "\n",
        "    # ========================================================================\n",
        "    # Example 2: Using custom loss class\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EXAMPLE 2: Model with custom Huber loss class\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    model2 = keras.Sequential([\n",
        "        keras.layers.Dense(64, activation=\"relu\", input_shape=(X_train_scaled.shape[1],)),\n",
        "        keras.layers.Dense(32, activation=\"relu\"),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    model2.compile(loss=HuberLoss(2.0), optimizer=\"adam\", metrics=[HuberMetric(2.0)])\n",
        "\n",
        "    history2 = model2.fit(X_train_scaled, y_train,\n",
        "                         validation_data=(X_test_scaled, y_test),\n",
        "                         epochs=5, batch_size=32, verbose=1)\n",
        "\n",
        "    # ========================================================================\n",
        "    # Example 3: Using custom layers\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EXAMPLE 3: Model with custom layers\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    model3 = keras.Sequential([\n",
        "        MyDense(64, activation=\"relu\"),\n",
        "        MyGaussianNoise(0.1),\n",
        "        MyDense(32, activation=\"relu\"),\n",
        "        MyDense(1)\n",
        "    ])\n",
        "\n",
        "    model3.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
        "    model3.fit(X_train_scaled, y_train,\n",
        "               validation_data=(X_test_scaled, y_test),\n",
        "               epochs=5, batch_size=32, verbose=1)\n",
        "\n",
        "    # ========================================================================\n",
        "    # Example 4: Using custom model (ResidualRegressor)\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EXAMPLE 4: Custom ResidualRegressor model\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    model4 = ResidualRegressor(output_dim=1)\n",
        "    model4.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
        "    model4.fit(X_train_scaled, y_train,\n",
        "               validation_data=(X_test_scaled, y_test),\n",
        "               epochs=5, batch_size=32, verbose=1)\n",
        "\n",
        "    # ========================================================================\n",
        "    # Example 5: Custom training loop\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EXAMPLE 5: Custom training loop\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Create a simple model for custom training\n",
        "    model5 = keras.Sequential([\n",
        "        keras.layers.Dense(64, activation=\"relu\", input_shape=(X_train_scaled.shape[1],)),\n",
        "        keras.layers.Dense(32, activation=\"relu\"),\n",
        "        keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Custom training parameters\n",
        "    n_epochs = 3\n",
        "    batch_size = 32\n",
        "    n_steps = len(X_train_scaled) // batch_size\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "    loss_fn = keras.losses.MeanSquaredError()\n",
        "\n",
        "    # Metrics\n",
        "    mean_loss = keras.metrics.Mean()\n",
        "    mae_metric = keras.metrics.MeanAbsoluteError()\n",
        "\n",
        "    # Custom training loop\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        print(f\"Epoch {epoch}/{n_epochs}\")\n",
        "\n",
        "        # Reset metrics\n",
        "        mean_loss.reset_state()\n",
        "        mae_metric.reset_state()\n",
        "\n",
        "        for step in range(1, n_steps + 1):\n",
        "            X_batch, y_batch = random_batch(X_train_scaled, y_train, batch_size)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                y_pred = model5(X_batch, training=True)\n",
        "                main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "                # Add regularization losses if any\n",
        "                loss = tf.add_n([main_loss] + model5.losses)\n",
        "\n",
        "            # Compute and apply gradients\n",
        "            gradients = tape.gradient(loss, model5.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, model5.trainable_variables))\n",
        "\n",
        "            # Update metrics\n",
        "            mean_loss.update_state(loss)\n",
        "            mae_metric.update_state(y_batch, y_pred)\n",
        "\n",
        "            # Print progress\n",
        "            if step % 10 == 0 or step == n_steps:\n",
        "                print_status_bar(step * batch_size, len(y_train), mean_loss, [mae_metric])\n",
        "\n",
        "        print()  # New line after each epoch\n",
        "\n",
        "    # ========================================================================\n",
        "    # Example 6: Gradient computation example\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EXAMPLE 6: Custom gradient computation\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Variables for gradient computation\n",
        "    w1 = tf.Variable(3.0)\n",
        "    w2 = tf.Variable(4.0)\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        z = f(w1, w2)\n",
        "\n",
        "    dz_dw1 = tape.gradient(z, w1)\n",
        "    dz_dw2 = tape.gradient(z, w2)\n",
        "\n",
        "    print(f\"f(w1={w1.numpy()}, w2={w2.numpy()}) = {z.numpy()}\")\n",
        "    print(f\"df/dw1 = {dz_dw1.numpy()}\")\n",
        "    print(f\"df/dw2 = {dz_dw2.numpy()}\")\n",
        "\n",
        "    # Clean up\n",
        "    del tape\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"All examples completed successfully!\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5TwNyoooQc-",
        "outputId": "e8516a6d-6e30-42cb-88d0-7415218da6ca"
      },
      "id": "J5TwNyoooQc-",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating synthetic dataset...\n",
            "Training data shape: (800, 10)\n",
            "Training labels shape: (800,)\n",
            "\n",
            "==================================================\n",
            "EXAMPLE 1: Model with custom Huber loss function\n",
            "==================================================\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 103.1421 - mae: 103.6409 - val_loss: 104.2953 - val_mae: 104.7953\n",
            "Epoch 2/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 102.7409 - mae: 103.2391 - val_loss: 103.7648 - val_mae: 104.2648\n",
            "Epoch 3/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 102.1120 - mae: 102.6104 - val_loss: 102.6833 - val_mae: 103.1833\n",
            "Epoch 4/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 100.8235 - mae: 101.3217 - val_loss: 100.5299 - val_mae: 101.0299\n",
            "Epoch 5/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 98.3268 - mae: 98.8260 - val_loss: 96.7364 - val_mae: 97.2358\n",
            "\n",
            "==================================================\n",
            "EXAMPLE 2: Model with custom Huber loss class\n",
            "==================================================\n",
            "Epoch 1/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - huber_metric_4: 6577.9766 - loss: 205.5618 - val_huber_metric_4: 6435.0059 - val_loss: 208.3313\n",
            "Epoch 2/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - huber_metric_4: 6575.4131 - loss: 205.4817 - val_huber_metric_4: 6434.3765 - val_loss: 208.3129\n",
            "Epoch 3/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - huber_metric_4: 6574.6206 - loss: 205.4569 - val_huber_metric_4: 6433.8408 - val_loss: 208.2973\n",
            "Epoch 4/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - huber_metric_4: 6573.9287 - loss: 205.4353 - val_huber_metric_4: 6433.2910 - val_loss: 208.2813\n",
            "Epoch 5/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - huber_metric_4: 6573.2261 - loss: 205.4133 - val_huber_metric_4: 6432.7407 - val_loss: 208.2650\n",
            "\n",
            "==================================================\n",
            "EXAMPLE 3: Model with custom layers\n",
            "==================================================\n",
            "Epoch 1/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 16987.4629 - mae: 103.6710 - val_loss: 17062.9922 - val_mae: 104.7902\n",
            "Epoch 2/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 16859.7812 - mae: 103.2648 - val_loss: 16885.2344 - val_mae: 104.2157\n",
            "Epoch 3/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 16643.4941 - mae: 102.5894 - val_loss: 16544.5625 - val_mae: 103.0913\n",
            "Epoch 4/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 16232.7607 - mae: 101.2720 - val_loss: 15924.0908 - val_mae: 100.9996\n",
            "Epoch 5/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 15518.3926 - mae: 98.9048 - val_loss: 14899.0625 - val_mae: 97.4531\n",
            "\n",
            "==================================================\n",
            "EXAMPLE 4: Custom ResidualRegressor model\n",
            "==================================================\n",
            "Epoch 1/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 16012.1475 - mae: 100.3637 - val_loss: 11997.6914 - val_mae: 87.4061\n",
            "Epoch 2/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9738.9385 - mae: 77.0751 - val_loss: 4267.3906 - val_mae: 52.5553\n",
            "Epoch 3/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3062.1531 - mae: 43.6400 - val_loss: 1875.8896 - val_mae: 32.6076\n",
            "Epoch 4/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1423.2067 - mae: 29.8453 - val_loss: 1254.0753 - val_mae: 26.6426\n",
            "Epoch 5/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 956.9491 - mae: 24.5764 - val_loss: 978.6442 - val_mae: 23.6873\n",
            "\n",
            "==================================================\n",
            "EXAMPLE 5: Custom training loop\n",
            "==================================================\n",
            "Epoch 1/3\n",
            "800/800 - mean: 16163.3887 - mean_absolute_error: 101.9922\n",
            "\n",
            "Epoch 2/3\n",
            "800/800 - mean: 3759.1794 - mean_absolute_error: 42.3064\n",
            "\n",
            "Epoch 3/3\n",
            "800/800 - mean: 324.0684 - mean_absolute_error: 13.8002\n",
            "\n",
            "\n",
            "==================================================\n",
            "EXAMPLE 6: Custom gradient computation\n",
            "==================================================\n",
            "f(w1=3.0, w2=4.0) = 51.0\n",
            "df/dw1 = 26.0\n",
            "df/dw2 = 6.0\n",
            "\n",
            "==================================================\n",
            "All examples completed successfully!\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}