{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import library yang diperlukan\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import (RandomForestClassifier, VotingClassifier,\n",
        "                            BaggingClassifier, AdaBoostClassifier,\n",
        "                            GradientBoostingRegressor, RandomForestRegressor)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "import xgboost as xgb\n",
        "\n",
        "# ===== CONTOH DATA =====\n",
        "# Membuat dataset contoh untuk klasifikasi\n",
        "X_class, y_class = make_classification(n_samples=1000, n_features=20,\n",
        "                                     n_informative=15, n_redundant=5,\n",
        "                                     random_state=42)\n",
        "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
        "    X_class, y_class, test_size=0.2, random_state=42)\n",
        "\n",
        "# Membuat dataset contoh untuk regresi\n",
        "X_reg, y_reg = make_regression(n_samples=1000, n_features=20,\n",
        "                              noise=0.1, random_state=42)\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Dataset berhasil dibuat!\")\n",
        "print(f\"Klasifikasi - Train: {X_train_class.shape}, Test: {X_test_class.shape}\")\n",
        "print(f\"Regresi - Train: {X_train_reg.shape}, Test: {X_test_reg.shape}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===== 1. VOTING CLASSIFIER =====\n",
        "print(\"1. VOTING CLASSIFIER\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Inisialisasi model individual\n",
        "log_clf = LogisticRegression(random_state=42)\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "svm_clf = SVC(probability=True, random_state=42)  # probability=True untuk soft voting\n",
        "\n",
        "# Hard Voting\n",
        "voting_clf_hard = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='hard'\n",
        ")\n",
        "voting_clf_hard.fit(X_train_class, y_train_class)\n",
        "y_pred_hard = voting_clf_hard.predict(X_test_class)\n",
        "accuracy_hard = accuracy_score(y_test_class, y_pred_hard)\n",
        "print(f\"Hard Voting Accuracy: {accuracy_hard:.4f}\")\n",
        "\n",
        "# Soft Voting\n",
        "voting_clf_soft = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='soft'\n",
        ")\n",
        "voting_clf_soft.fit(X_train_class, y_train_class)\n",
        "y_pred_soft = voting_clf_soft.predict(X_test_class)\n",
        "accuracy_soft = accuracy_score(y_test_class, y_pred_soft)\n",
        "print(f\"Soft Voting Accuracy: {accuracy_soft:.4f}\")\n",
        "\n",
        "# Bandingkan dengan model individual\n",
        "for clf_name, clf in voting_clf_soft.named_estimators_.items():\n",
        "    clf.fit(X_train_class, y_train_class)\n",
        "    y_pred_individual = clf.predict(X_test_class)\n",
        "    accuracy_individual = accuracy_score(y_test_class, y_pred_individual)\n",
        "    print(f\"{clf_name} Individual Accuracy: {accuracy_individual:.4f}\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===== 2. BAGGING CLASSIFIER =====\n",
        "print(\"2. BAGGING CLASSIFIER\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Bagging dengan Decision Tree\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=500,\n",
        "    max_samples=100,\n",
        "    bootstrap=True,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "bag_clf.fit(X_train_class, y_train_class)\n",
        "y_pred_bag = bag_clf.predict(X_test_class)\n",
        "accuracy_bag = accuracy_score(y_test_class, y_pred_bag)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bag:.4f}\")\n",
        "\n",
        "# Bandingkan dengan single Decision Tree\n",
        "single_tree = DecisionTreeClassifier(random_state=42)\n",
        "single_tree.fit(X_train_class, y_train_class)\n",
        "y_pred_single = single_tree.predict(X_test_class)\n",
        "accuracy_single = accuracy_score(y_test_class, y_pred_single)\n",
        "print(f\"Single Decision Tree Accuracy: {accuracy_single:.4f}\")\n",
        "print(f\"Improvement: {accuracy_bag - accuracy_single:.4f}\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===== 3. RANDOM FOREST =====\n",
        "print(\"3. RANDOM FOREST\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rnd_clf = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    max_leaf_nodes=16,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "rnd_clf.fit(X_train_class, y_train_class)\n",
        "y_pred_rf = rnd_clf.predict(X_test_class)\n",
        "accuracy_rf = accuracy_score(y_test_class, y_pred_rf)\n",
        "print(f\"Random Forest Accuracy: {accuracy_rf:.4f}\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = rnd_clf.feature_importances_\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "for i, importance in enumerate(sorted(enumerate(feature_importance),\n",
        "                                    key=lambda x: x[1], reverse=True)[:5]):\n",
        "    print(f\"Feature {importance[0]}: {importance[1]:.4f}\")\n",
        "\n",
        "# Extra Trees (Extremely Randomized Trees)\n",
        "bag_clf_extra = BaggingClassifier(\n",
        "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16, random_state=42),\n",
        "    n_estimators=500,\n",
        "    max_samples=1.0,\n",
        "    bootstrap=True,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "bag_clf_extra.fit(X_train_class, y_train_class)\n",
        "y_pred_extra = bag_clf_extra.predict(X_test_class)\n",
        "accuracy_extra = accuracy_score(y_test_class, y_pred_extra)\n",
        "print(f\"Extra Trees Accuracy: {accuracy_extra:.4f}\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===== 4. ADABOOST =====\n",
        "print(\"4. ADABOOST\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1, random_state=42),\n",
        "    n_estimators=200,\n",
        "    algorithm=\"SAMME\",\n",
        "    learning_rate=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "ada_clf.fit(X_train_class, y_train_class)\n",
        "y_pred_ada = ada_clf.predict(X_test_class)\n",
        "accuracy_ada = accuracy_score(y_test_class, y_pred_ada)\n",
        "print(f\"AdaBoost Accuracy: {accuracy_ada:.4f}\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===== 5. GRADIENT BOOSTING (Manual Implementation) =====\n",
        "print(\"5. GRADIENT BOOSTING - Manual Implementation\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Manual Gradient Boosting untuk Regresi\n",
        "def manual_gradient_boosting(X, y, X_new, max_depth=2, n_estimators=3):\n",
        "    \"\"\"\n",
        "    Implementasi manual gradient boosting\n",
        "    \"\"\"\n",
        "    trees = []\n",
        "    predictions = []\n",
        "\n",
        "    # First tree\n",
        "    tree_reg1 = DecisionTreeRegressor(max_depth=max_depth, random_state=42)\n",
        "    tree_reg1.fit(X, y)\n",
        "    trees.append(tree_reg1)\n",
        "\n",
        "    # Residual untuk tree kedua\n",
        "    y_residual = y - tree_reg1.predict(X)\n",
        "    tree_reg2 = DecisionTreeRegressor(max_depth=max_depth, random_state=42)\n",
        "    tree_reg2.fit(X, y_residual)\n",
        "    trees.append(tree_reg2)\n",
        "\n",
        "    # Residual untuk tree ketiga\n",
        "    y_residual2 = y_residual - tree_reg2.predict(X)\n",
        "    tree_reg3 = DecisionTreeRegressor(max_depth=max_depth, random_state=42)\n",
        "    tree_reg3.fit(X, y_residual2)\n",
        "    trees.append(tree_reg3)\n",
        "\n",
        "    # Prediksi final\n",
        "    y_pred = sum(tree.predict(X_new) for tree in trees)\n",
        "    return y_pred, trees\n",
        "\n",
        "# Test manual implementation\n",
        "y_pred_manual, manual_trees = manual_gradient_boosting(\n",
        "    X_train_reg, y_train_reg, X_test_reg)\n",
        "mse_manual = mean_squared_error(y_test_reg, y_pred_manual)\n",
        "print(f\"Manual Gradient Boosting MSE: {mse_manual:.4f}\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===== 6. GRADIENT BOOSTING (Sklearn) =====\n",
        "print(\"6. GRADIENT BOOSTING - Sklearn Implementation\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Basic Gradient Boosting\n",
        "gbrt = GradientBoostingRegressor(\n",
        "    max_depth=2,\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "gbrt.fit(X_train_reg, y_train_reg)\n",
        "y_pred_gbrt = gbrt.predict(X_test_reg)\n",
        "mse_gbrt = mean_squared_error(y_test_reg, y_pred_gbrt)\n",
        "print(f\"Gradient Boosting MSE: {mse_gbrt:.4f}\")\n",
        "\n",
        "# Gradient Boosting dengan Early Stopping\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train_reg, y_train_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "gbrt_early = GradientBoostingRegressor(\n",
        "    max_depth=2,\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "gbrt_early.fit(X_train_split, y_train_split)\n",
        "\n",
        "# Mencari estimator terbaik\n",
        "errors = [mean_squared_error(y_val_split, y_pred)\n",
        "          for y_pred in gbrt_early.staged_predict(X_val_split)]\n",
        "best_n_estimators = np.argmin(errors) + 1\n",
        "\n",
        "print(f\"Best number of estimators: {best_n_estimators}\")\n",
        "\n",
        "# Training ulang dengan estimator terbaik\n",
        "gbrt_best = GradientBoostingRegressor(\n",
        "    max_depth=2,\n",
        "    n_estimators=best_n_estimators,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "gbrt_best.fit(X_train_reg, y_train_reg)\n",
        "y_pred_best = gbrt_best.predict(X_test_reg)\n",
        "mse_best = mean_squared_error(y_test_reg, y_pred_best)\n",
        "print(f\"Optimized Gradient Boosting MSE: {mse_best:.4f}\")\n",
        "\n",
        "# Early Stopping dengan warm_start\n",
        "gbrt_warm = GradientBoostingRegressor(\n",
        "    max_depth=2,\n",
        "    warm_start=True,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "min_val_error = float(\"inf\")\n",
        "error_going_up = 0\n",
        "best_estimators_warm = 0\n",
        "\n",
        "for n_estimators in range(1, 200):\n",
        "    gbrt_warm.n_estimators = n_estimators\n",
        "    gbrt_warm.fit(X_train_split, y_train_split)\n",
        "    y_pred_val = gbrt_warm.predict(X_val_split)\n",
        "    val_error = mean_squared_error(y_val_split, y_pred_val)\n",
        "\n",
        "    if val_error < min_val_error:\n",
        "        min_val_error = val_error\n",
        "        error_going_up = 0\n",
        "        best_estimators_warm = n_estimators\n",
        "    else:\n",
        "        error_going_up += 1\n",
        "        if error_going_up == 5:  # early stopping\n",
        "            break\n",
        "\n",
        "print(f\"Early stopping at {best_estimators_warm} estimators\")\n",
        "print(f\"Best validation error: {min_val_error:.4f}\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===== 7. XGBOOST =====\n",
        "print(\"7. XGBOOST\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Basic XGBoost\n",
        "xgb_reg = xgb.XGBRegressor(random_state=42)\n",
        "xgb_reg.fit(X_train_reg, y_train_reg)\n",
        "y_pred_xgb = xgb_reg.predict(X_test_reg)\n",
        "mse_xgb = mean_squared_error(y_test_reg, y_pred_xgb)\n",
        "print(f\"XGBoost MSE: {mse_xgb:.4f}\")\n",
        "\n",
        "# XGBoost dengan Early Stopping (versi baru)\n",
        "try:\n",
        "    # Untuk XGBoost versi baru (>= 1.6.0)\n",
        "    xgb_reg_early = xgb.XGBRegressor(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.1,\n",
        "        early_stopping_rounds=10,\n",
        "        eval_metric='rmse',\n",
        "        random_state=42\n",
        "    )\n",
        "    xgb_reg_early.fit(\n",
        "        X_train_split, y_train_split,\n",
        "        eval_set=[(X_val_split, y_val_split)],\n",
        "        verbose=False\n",
        "    )\n",
        "    y_pred_xgb_early = xgb_reg_early.predict(X_test_reg)\n",
        "    mse_xgb_early = mean_squared_error(y_test_reg, y_pred_xgb_early)\n",
        "    print(f\"XGBoost with Early Stopping MSE: {mse_xgb_early:.4f}\")\n",
        "    try:\n",
        "        print(f\"Best iteration: {xgb_reg_early.best_iteration}\")\n",
        "    except:\n",
        "        print(\"Best iteration info not available\")\n",
        "\n",
        "except Exception as e:\n",
        "    # Fallback untuk XGBoost versi lama\n",
        "    print(f\"XGBoost early stopping error: {e}\")\n",
        "    print(\"Using basic XGBoost without early stopping...\")\n",
        "    xgb_reg_early = xgb.XGBRegressor(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42\n",
        "    )\n",
        "    xgb_reg_early.fit(X_train_split, y_train_split)\n",
        "    y_pred_xgb_early = xgb_reg_early.predict(X_test_reg)\n",
        "    mse_xgb_early = mean_squared_error(y_test_reg, y_pred_xgb_early)\n",
        "    print(f\"XGBoost (fallback) MSE: {mse_xgb_early:.4f}\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===== SUMMARY PERBANDINGAN =====\n",
        "print(\"SUMMARY - MODEL COMPARISON\")\n",
        "print(\"-\" * 40)\n",
        "print(\"CLASSIFICATION MODELS:\")\n",
        "print(f\"Hard Voting:        {accuracy_hard:.4f}\")\n",
        "print(f\"Soft Voting:        {accuracy_soft:.4f}\")\n",
        "print(f\"Bagging:            {accuracy_bag:.4f}\")\n",
        "print(f\"Random Forest:      {accuracy_rf:.4f}\")\n",
        "print(f\"Extra Trees:        {accuracy_extra:.4f}\")\n",
        "print(f\"AdaBoost:           {accuracy_ada:.4f}\")\n",
        "print(f\"Single Tree:        {accuracy_single:.4f}\")\n",
        "\n",
        "print(\"\\nREGRESSION MODELS (MSE):\")\n",
        "print(f\"Manual Gradient Boosting:    {mse_manual:.4f}\")\n",
        "print(f\"Gradient Boosting:           {mse_gbrt:.4f}\")\n",
        "print(f\"Optimized Gradient Boosting: {mse_best:.4f}\")\n",
        "print(f\"XGBoost:                     {mse_xgb:.4f}\")\n",
        "print(f\"XGBoost Early Stopping:      {mse_xgb_early:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"KODE SELESAI - SEMUA MODEL BERHASIL DILATIH!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-KC467CWAyI",
        "outputId": "9d05de68-1b42-4ede-ad0d-fde4f603046a"
      },
      "id": "V-KC467CWAyI",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset berhasil dibuat!\n",
            "Klasifikasi - Train: (800, 20), Test: (200, 20)\n",
            "Regresi - Train: (800, 20), Test: (200, 20)\n",
            "============================================================\n",
            "1. VOTING CLASSIFIER\n",
            "------------------------------\n",
            "Hard Voting Accuracy: 0.9100\n",
            "Soft Voting Accuracy: 0.9100\n",
            "lr Individual Accuracy: 0.8250\n",
            "rf Individual Accuracy: 0.9000\n",
            "svc Individual Accuracy: 0.9350\n",
            "============================================================\n",
            "2. BAGGING CLASSIFIER\n",
            "------------------------------\n",
            "Bagging Classifier Accuracy: 0.8750\n",
            "Single Decision Tree Accuracy: 0.7900\n",
            "Improvement: 0.0850\n",
            "============================================================\n",
            "3. RANDOM FOREST\n",
            "------------------------------\n",
            "Random Forest Accuracy: 0.8700\n",
            "Top 5 Most Important Features:\n",
            "Feature 12: 0.1731\n",
            "Feature 2: 0.0930\n",
            "Feature 5: 0.0727\n",
            "Feature 17: 0.0694\n",
            "Feature 6: 0.0686\n",
            "Extra Trees Accuracy: 0.8700\n",
            "============================================================\n",
            "4. ADABOOST\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Accuracy: 0.8100\n",
            "============================================================\n",
            "5. GRADIENT BOOSTING - Manual Implementation\n",
            "------------------------------\n",
            "Manual Gradient Boosting MSE: 20303.1623\n",
            "============================================================\n",
            "6. GRADIENT BOOSTING - Sklearn Implementation\n",
            "------------------------------\n",
            "Gradient Boosting MSE: 3964.4760\n",
            "Best number of estimators: 200\n",
            "Optimized Gradient Boosting MSE: 2044.9120\n",
            "Early stopping at 199 estimators\n",
            "Best validation error: 2931.7191\n",
            "============================================================\n",
            "7. XGBOOST\n",
            "------------------------------\n",
            "XGBoost MSE: 5530.4742\n",
            "XGBoost with Early Stopping MSE: 5498.4452\n",
            "Best iteration: 198\n",
            "============================================================\n",
            "SUMMARY - MODEL COMPARISON\n",
            "----------------------------------------\n",
            "CLASSIFICATION MODELS:\n",
            "Hard Voting:        0.9100\n",
            "Soft Voting:        0.9100\n",
            "Bagging:            0.8750\n",
            "Random Forest:      0.8700\n",
            "Extra Trees:        0.8700\n",
            "AdaBoost:           0.8100\n",
            "Single Tree:        0.7900\n",
            "\n",
            "REGRESSION MODELS (MSE):\n",
            "Manual Gradient Boosting:    20303.1623\n",
            "Gradient Boosting:           3964.4760\n",
            "Optimized Gradient Boosting: 2044.9120\n",
            "XGBoost:                     5530.4742\n",
            "XGBoost Early Stopping:      5498.4452\n",
            "\n",
            "============================================================\n",
            "KODE SELESAI - SEMUA MODEL BERHASIL DILATIH!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}